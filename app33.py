
import streamlit as st
import pandas as pd
from supabase import create_client, Client
import requests
import re
from datetime import datetime, timedelta
import json

# 1. Supabase ì„¤ì •
URL = "https://qipphcdzlmqidhrjnjtt.supabase.co"
KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFpcHBoY2R6bG1xaWRocmpuanR0Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjY5NTIwMTIsImV4cCI6MjA4MjUyODAxMn0.AsuvjVGCLUJF_IPvQevYASaM6uRF2C6F-CjwC3eCNVk"

try:
    supabase: Client = create_client(URL, KEY)
except Exception as e:
    st.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
    st.stop()

# 2. ê³ ì • ì„¤ì •
CAPA_INFO = {"ì¡°ë¦½1": 3300, "ì¡°ë¦½2": 3700, "ì¡°ë¦½3": 3600}
CAPA_90_PERCENT = {"ì¡°ë¦½1": 2970, "ì¡°ë¦½2": 3330, "ì¡°ë¦½3": 3240}

WEEKDAY_RULES = {
    "ì¡°ë¦½2": {
        "ì›”ìš”ì¼": ["FAN", "MOTOR"],
        "í™”ìš”ì¼": ["FLANGE", "MOTOR"],
        "ìˆ˜ìš”ì¼": ["FAN", "MOTOR"],
        "ëª©ìš”ì¼": ["FLANGE", "MOTOR"],
        "ê¸ˆìš”ì¼": ["FAN", "MOTOR"],
    }
}

FEW_SHOT_EXAMPLES = """
## ğŸ“š ì°¸ê³ í•  ì„±ê³µ ì‚¬ë¡€

### ì‚¬ë¡€ 1: 2025ë…„ 10ì›” 15ì¼ ì¡°ë¦½2 CAPA ì´ˆê³¼
**í•´ê²°**: ì¡°ë¦½2 â†’ ì¡°ë¦½1ë¡œ 500ê°œ ì´ë™, ë‹¬ì„±ë¥  98.5%

### ì‚¬ë¡€ 2: 2025ë…„ 11ì›” 8ì¼ ìš”ì¼ê·œì¹™ ìœ„ë°˜
**í•´ê²°**: FAN í’ˆëª©ì„ ëª©ìš”ì¼ â†’ ìˆ˜ìš”ì¼ë¡œ ì´ë™, ë‹¬ì„±ë¥  99.2%
"""

# --- ë°ì´í„° ë¡œë“œ ---
@st.cache_data(ttl=600)
def fetch_data(target_date=None):
    try:
        hist_res = supabase.table("production_issue_analysis_8_11")\
            .select("ìµœì¢…_ì´ìŠˆë¶„ë¥˜, í’ˆëª©ëª…, ë¼ì¸, ë‚ ì§œ, ëˆ„ì ë‹¬ì„±ë¥ ")\
            .execute()
        hist_df = pd.DataFrame(hist_res.data)

        if target_date:
            dt = datetime.strptime(target_date, '%Y-%m-%d')
            start_date = (dt - timedelta(days=2)).strftime('%Y-%m-%d')
            end_date = (dt + timedelta(days=2)).strftime('%Y-%m-%d')
            
            plan_res = supabase.table("production_plan_2026_01")\
                .select("*")\
                .gte("plan_date", start_date)\
                .lte("plan_date", end_date)\
                .execute()
            plan_df = pd.DataFrame(plan_res.data)
            
            if not plan_df.empty:
                plan_df = analyze_plan_issues(plan_df)
        else:
            plan_df = pd.DataFrame()

        return hist_df, plan_df
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame(), pd.DataFrame()

# --- ì‚¬ì „ ì´ìŠˆ íƒì§€ ---
def analyze_plan_issues(df):
    if df.empty:
        return df
    
    issues = []
    
    for date, group in df.groupby('plan_date'):
        dt = datetime.strptime(date, '%Y-%m-%d')
        weekday = dt.strftime('%A')
        weekday_kr = {'Monday': 'ì›”ìš”ì¼', 'Tuesday': 'í™”ìš”ì¼', 'Wednesday': 'ìˆ˜ìš”ì¼',
                      'Thursday': 'ëª©ìš”ì¼', 'Friday': 'ê¸ˆìš”ì¼', 'Saturday': 'í† ìš”ì¼', 'Sunday': 'ì¼ìš”ì¼'}.get(weekday, weekday)
        
        for line in group['line'].unique():
            line_data = group[group['line'] == line]
            total_qty = line_data['qty_0ì°¨'].sum() if 'qty_0ì°¨' in line_data.columns else 0
            
            if total_qty > CAPA_90_PERCENT.get(line, 9999):
                issues.append({
                    'date': date,
                    'line': line,
                    'issue_type': 'CAPA_ì´ˆê³¼',
                    'current_qty': int(total_qty),
                    'max_qty': CAPA_90_PERCENT[line],
                    'over_qty': int(total_qty - CAPA_90_PERCENT[line])
                })
            
            if line == 'ì¡°ë¦½2' and weekday_kr in WEEKDAY_RULES['ì¡°ë¦½2']:
                allowed_products = WEEKDAY_RULES['ì¡°ë¦½2'][weekday_kr]
                for _, row in line_data.iterrows():
                    product = str(row.get('product_name', ''))
                    is_allowed = any(allowed in product.upper() for allowed in allowed_products)
                    if not is_allowed:
                        issues.append({
                            'date': date,
                            'line': line,
                            'issue_type': 'ìš”ì¼ê·œì¹™_ìœ„ë°˜',
                            'weekday': weekday_kr,
                            'product': product,
                            'allowed': allowed_products,
                            'qty': int(row.get('qty_0ì°¨', 0))
                        })
            
            if line == 'ì¡°ë¦½2' and len(line_data) > 5:
                issues.append({
                    'date': date,
                    'line': line,
                    'issue_type': 'í’ˆëª©ìˆ˜_ì´ˆê³¼',
                    'current_count': len(line_data),
                    'max_count': 5,
                    'products': list(line_data['product_name'].values)
                })
    
    df['detected_issues'] = json.dumps(issues, ensure_ascii=False) if issues else '[]'
    return df

# --- RAG: ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ---
def retrieve_similar_cases(history_df, current_issues):
    if history_df.empty or not current_issues:
        return "ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ"
    
    issue_types = set()
    for issue in current_issues:
        if issue['issue_type'] == 'CAPA_ì´ˆê³¼':
            issue_types.add('CAPA')
        elif issue['issue_type'] == 'ìš”ì¼ê·œì¹™_ìœ„ë°˜':
            issue_types.add('ìš”ì¼')
        elif issue['issue_type'] == 'í’ˆëª©ìˆ˜_ì´ˆê³¼':
            issue_types.add('í’ˆëª©')
    
    similar_cases = []
    for issue_type in issue_types:
        matched = history_df[history_df['ìµœì¢…_ì´ìŠˆë¶„ë¥˜'].str.contains(issue_type, na=False, case=False)]
        if not matched.empty:
            top_cases = matched.nlargest(3, 'ëˆ„ì ë‹¬ì„±ë¥ ') if 'ëˆ„ì ë‹¬ì„±ë¥ ' in matched.columns else matched.head(3)
            similar_cases.append(f"\n### {issue_type} ê´€ë ¨ ê³¼ê±° ì‚¬ë¡€")
            for idx, row in top_cases.iterrows():
                similar_cases.append(f"- ë‚ ì§œ: {row.get('ë‚ ì§œ', 'N/A')}, í’ˆëª©: {row.get('í’ˆëª©ëª…', 'N/A')}, "
                                   f"ë¼ì¸: {row.get('ë¼ì¸', 'N/A')}, ë‹¬ì„±ë¥ : {row.get('ëˆ„ì ë‹¬ì„±ë¥ ', 'N/A')}%")
    
    return "\n".join(similar_cases) if similar_cases else "ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ"

# --- AI ë‹µë³€ ê²€ì¦ ---
def validate_ai_response(response, current_df):
    if current_df.empty:
        return True, [], "âœ… ê²€ì¦í•  ë°ì´í„° ì—†ìŒ"
    
    warnings = []
    details = []
    
    mentioned_dates = set()
    dates_pattern1 = re.findall(r'202[56]-\d{2}-\d{2}', response)
    mentioned_dates.update(dates_pattern1)
    
    dates_pattern2 = re.findall(r'(\d{1,2})/(\d{1,2})', response)
    for m, d in dates_pattern2:
        mentioned_dates.add(f"2026-{int(m):02d}-{int(d):02d}")
    
    dates_pattern3 = re.findall(r'(\d{1,2})ì›”\s*(\d{1,2})ì¼', response)
    for m, d in dates_pattern3:
        mentioned_dates.add(f"2026-{int(m):02d}-{int(d):02d}")
    
    actual_dates = set(current_df['plan_date'].unique())
    invalid_dates = mentioned_dates - actual_dates
    
    if invalid_dates:
        warnings.append({
            'type': 'DATE_MISMATCH',
            'severity': 'HIGH',
            'message': f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‚ ì§œ: {', '.join(sorted(invalid_dates))}"
        })
        details.append(f"âŒ **ë‚ ì§œ ì˜¤ë¥˜**: {', '.join(sorted(invalid_dates))}")
    else:
        details.append(f"âœ… **ë‚ ì§œ ê²€ì¦**: í†µê³¼ ({len(mentioned_dates)}ê°œ)")
    
    mentioned_qtys = re.findall(r'\b([1-9]\d{2,})\b', response)
    mentioned_qtys = [int(q) for q in mentioned_qtys]
    
    actual_qtys = set()
    if 'qty_0ì°¨' in current_df.columns:
        actual_qtys = set(current_df['qty_0ì°¨'].dropna().astype(int))
    
    suspicious_qtys = [q for q in mentioned_qtys if q not in actual_qtys and q > 100]
    
    if len(suspicious_qtys) > 3:
        warnings.append({
            'type': 'QUANTITY_SUSPICIOUS',
            'severity': 'MEDIUM',
            'message': f"ì˜ì‹¬ ìˆ˜ëŸ‰ {len(suspicious_qtys)}ê°œ"
        })
        details.append(f"âš ï¸ **ìˆ˜ëŸ‰ ì˜ì‹¬**: ì¼ë¶€ ë¶ˆì¼ì¹˜")
    else:
        details.append(f"âœ… **ìˆ˜ëŸ‰ ê²€ì¦**: í†µê³¼")
    
    after_qtys = re.findall(r'ë³€ê²½\s*í›„\s*ìˆ˜ëŸ‰[:\s]+(\d+)', response)
    after_qtys = [int(q) for q in after_qtys]
    
    capa_violations = []
    for qty in after_qtys:
        for line, max_capa in CAPA_90_PERCENT.items():
            if qty > max_capa:
                capa_violations.append(f"{line} ì´ˆê³¼: {qty} > {max_capa}")
    
    if capa_violations:
        warnings.append({
            'type': 'CAPA_VIOLATION',
            'severity': 'CRITICAL',
            'message': f"CAPA ìœ„ë°˜: {capa_violations[0]}"
        })
        details.append(f"ğŸš¨ **CAPA ìœ„ë°˜**: {capa_violations[0]}")
    else:
        details.append(f"âœ… **CAPA ê²€ì¦**: í†µê³¼")
    
    critical_warnings = [w for w in warnings if w['severity'] == 'CRITICAL']
    high_warnings = [w for w in warnings if w['severity'] == 'HIGH']
    is_valid = len(critical_warnings) == 0 and len(high_warnings) <= 1
    
    validation_report = "\n".join(details)
    if warnings:
        validation_report += "\n\n### âš ï¸ ê²½ê³ \n"
        for w in warnings:
            severity_icon = {'CRITICAL': 'ğŸš¨', 'HIGH': 'âŒ', 'MEDIUM': 'âš ï¸'}.get(w['severity'], 'âš ï¸')
            validation_report += f"{severity_icon} {w['message']}\n"
    
    return is_valid, warnings, validation_report

# --- AI ë¶„ì„ ì—”ì§„ (ìˆ˜ì •) ---
def ask_professional_scheduler(question, current_df, history_df):
    api_url = "https://ai.potens.ai/api/chat"
    api_key = "qD2gfuVAkMJexDAcFb5GnEb1SZksTs7o"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

    if not current_df.empty:
        # â­ 1. ë‚ ì§œë³„ ë¼ì¸ë³„ ì •í™•í•œ ì§‘ê³„
        summary = current_df.groupby(['plan_date', 'line']).agg({
            'qty_0ì°¨': 'sum',
            'product_name': 'count'
        }).reset_index()
        summary.columns = ['plan_date', 'line', 'total_qty', 'product_count']
        
        # â­ 2. ìƒì„¸ í’ˆëª© ë¦¬ìŠ¤íŠ¸ (ë³„ë„)
        product_details = current_df.groupby(['plan_date', 'line']).apply(
            lambda x: x[['product_name', 'qty_0ì°¨']].to_dict('records')
        ).reset_index()
        product_details.columns = ['plan_date', 'line', 'products']
        
        # â­ 3. í•©ì¹˜ê¸°
        summary = summary.merge(product_details, on=['plan_date', 'line'])
        
        # â­ 4. ëª…í™•í•œ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        data_text = ""
        for _, row in summary.iterrows():
            data_text += f"\n## {row['plan_date']} / {row['line']}\n"
            data_text += f"**ì´ ê³„íš ìˆ˜ëŸ‰: {int(row['total_qty'])}ê°œ** (í’ˆëª© ìˆ˜: {int(row['product_count'])}ê°œ)\n"
            data_text += f"**CAPA 90% ê¸°ì¤€: {CAPA_90_PERCENT.get(row['line'], 'N/A')}ê°œ**\n"
            
            if row['total_qty'] > CAPA_90_PERCENT.get(row['line'], 99999):
                over = int(row['total_qty'] - CAPA_90_PERCENT.get(row['line'], 0))
                data_text += f"âš ï¸ **CAPA ì´ˆê³¼: {over}ê°œ ì´ˆê³¼**\n"
            
            data_text += "\nìƒì„¸ í’ˆëª©:\n"
            for prod in row['products']:
                data_text += f"  - {prod['product_name']}: {int(prod['qty_0ì°¨'])}ê°œ\n"
        
        detected_issues_str = current_df['detected_issues'].iloc[0] if 'detected_issues' in current_df.columns else '[]'
        detected_issues = json.loads(detected_issues_str)
    else:
        data_text = "ë°ì´í„° ì—†ìŒ"
        detected_issues = []
    
    similar_cases = retrieve_similar_cases(history_df, detected_issues)
    
    system_rules = f"""
ë‹¹ì‹ ì€ ìë™ì°¨ ë¶€í’ˆ ì¡°ë¦½ë¼ì¸ì˜ 'ìˆ˜ì„ ìƒì‚° ìŠ¤ì¼€ì¤„ëŸ¬'ì…ë‹ˆë‹¤.

## âš ï¸ ì ˆëŒ€ ê·œì¹™
1. **ì•„ë˜ [í˜„ì¬ 1ì›” ê³„íš ë°ì´í„°]ì— ëª…ì‹œëœ "ì´ ê³„íš ìˆ˜ëŸ‰"ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”**
2. ìˆ«ìë¥¼ ì ˆëŒ€ ì„ì˜ë¡œ ê³„ì‚°í•˜ê±°ë‚˜ ì¶”ì •í•˜ì§€ ë§ˆì„¸ìš”
3. ì œê³µëœ ìˆ«ìë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”

## ğŸ“Š í˜„ì¬ 1ì›” ê³„íš ë°ì´í„° (ì •í™•í•œ ì§‘ê³„)
{data_text}

## ğŸš¨ ì‚¬ì „ íƒì§€ ì´ìŠˆ
{json.dumps(detected_issues, ensure_ascii=False, indent=2)}

## ğŸ“š ìœ ì‚¬ ê³¼ê±° ì‚¬ë¡€
{similar_cases}

{FEW_SHOT_EXAMPLES}

## ğŸ“ í•„ìˆ˜ ê·œì¹™
1. CAPA 90%: ì¡°ë¦½1={CAPA_90_PERCENT['ì¡°ë¦½1']}, ì¡°ë¦½2={CAPA_90_PERCENT['ì¡°ë¦½2']}, ì¡°ë¦½3={CAPA_90_PERCENT['ì¡°ë¦½3']}
2. ì¡°ë¦½2 ìš”ì¼ì œ: {json.dumps(WEEKDAY_RULES['ì¡°ë¦½2'], ensure_ascii=False)}
3. ì¡°ë¦½2 í’ˆëª©: í•˜ë£¨ ìµœëŒ€ 5í’ˆëª©

## ğŸ“ ì¶œë ¥ í˜•ì‹

### ëŒ€ì•ˆ 1: [ì œëª©]

**ğŸ”§ ì¡°ì¹˜ì‚¬í•­**
- ë‚ ì§œ: [ì‹¤ì œ ë‚ ì§œ]
- ë¼ì¸: [ì¡°ë¦½1/2/3]
- í˜„ì¬ ìƒí™©: **ìœ„ ë°ì´í„°ì˜ "ì´ ê³„íš ìˆ˜ëŸ‰"ì„ ê·¸ëŒ€ë¡œ ì¸ìš©**
- ì œì•ˆ: [êµ¬ì²´ì  ë³€ê²½ ë‚´ìš©]

**ğŸ“Š ê·¼ê±°**
- ê·œì¹™: [ë²ˆí˜¸]
- í˜„ì¬ ìˆ˜ëŸ‰: [ìœ„ ë°ì´í„° ì§ì ‘ ì¸ìš©]
- ì´ˆê³¼ëŸ‰: [ìœ„ ë°ì´í„° ì§ì ‘ ì¸ìš©]

**âœ… ì¥ì  / âš ï¸ ë‹¨ì **

---
(ëŒ€ì•ˆ 2, 3 ë™ì¼)

âš ï¸ ì£¼ì˜: ìˆ«ìëŠ” ìœ„ [í˜„ì¬ 1ì›” ê³„íš ë°ì´í„°]ì˜ "ì´ ê³„íš ìˆ˜ëŸ‰"ì„ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
"""

    payload = {
        "prompt": f"{system_rules}\n\n## ì‚¬ìš©ì ìš”ì²­\n{question}",
        "temperature": 0.1,
        "max_tokens": 2500
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=90)
        response.raise_for_status()
        ai_response = response.json().get('message', 'ì‘ë‹µ ìƒì„± ì˜¤ë¥˜')
        
        is_valid, warnings, validation_report = validate_ai_response(ai_response, current_df)
        
        if not is_valid:
            ai_response += f"\n\n---\n## ğŸ” ê²€ì¦ ê²°ê³¼\n{validation_report}"
        
        return ai_response, is_valid, warnings, validation_report
        
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {str(e)}", False, [], ""

# --- ë‚ ì§œ ì¶”ì¶œ ---
def extract_date(text):
    patterns = [r'(\d{1,2})/(\d{1,2})', r'(\d{1,2})ì›”\s*(\d{1,2})ì¼']
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            m, d = match.groups()
            return f"2026-{int(m):02d}-{int(d):02d}"
    return None

# --- ë©”ì¸ UI ---
st.set_page_config(page_title="AI ìˆ˜ì„ ìŠ¤ì¼€ì¤„ëŸ¬", layout="wide")
st.title("ğŸ‘¨â€âœˆï¸ ìˆ˜ì„ ìŠ¤ì¼€ì¤„ëŸ¬ AI í†µí•© ì „ëµ ê´€ì œ (2026.01)")

with st.sidebar:
    st.header("âš™ï¸ ìƒì‚° ë¼ì¸ CAPA")
    st.json(CAPA_INFO)
    st.subheader("ğŸ“ CAPA 90%")
    st.json(CAPA_90_PERCENT)
    st.subheader("ğŸ“… ì¡°ë¦½2 ìš”ì¼ ê·œì¹™")
    st.json(WEEKDAY_RULES)
    if st.button("ğŸ”„ ë°ì´í„° ë™ê¸°í™”"):
        st.cache_data.clear()
        st.rerun()

if "messages" not in st.session_state:
    st.session_state.messages = []

if "target_date" not in st.session_state:
    st.session_state.target_date = None

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): 
        st.markdown(msg["content"])

if prompt := st.chat_input("ì´ìŠˆë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1/14 ì¡°ë¦½2 FAN ìš”ì¼ìœ„ë°˜ í•´ê²°í•´ì¤˜)"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): 
        st.markdown(prompt)

    target_date = extract_date(prompt)
    st.session_state.target_date = target_date
    
    with st.spinner("ğŸš€ ë¶„ì„ ì¤‘..."):
        history_df, current_plan = fetch_data(target_date)
        answer, is_valid, warnings, validation_report = ask_professional_scheduler(prompt, current_plan, history_df)
        
        st.session_state.messages.append({"role": "assistant", "content": answer})
        
        with st.chat_message("assistant"):
            st.markdown(answer)
            
            if is_valid:
                st.success("âœ… AI ë‹µë³€ ê²€ì¦ í†µê³¼")
            else:
                st.warning("âš ï¸ ì¼ë¶€ ë¶ˆì¼ì¹˜ ë°œê²¬")
            
            with st.expander("ğŸ” ìƒì„¸ ê²€ì¦ ê²°ê³¼"):
                st.markdown(validation_report)
            
            col1, col2 = st.columns(2)
            with col1:
                if not current_plan.empty:
                    with st.expander("ğŸ“ 1ì›” ê³„íš ì›ë³¸"):
                        display_df = current_plan.drop(columns=['detected_issues'], errors='ignore')
                        st.dataframe(display_df)
            
            with col2:
                if not history_df.empty:
                    with st.expander("ğŸ“š ê³¼ê±° ì´ìŠˆ Top 5"):
                        issue_summary = history_df['ìµœì¢…_ì´ìŠˆë¶„ë¥˜'].value_counts().head(5)
                        st.bar_chart(issue_summary)

with st.expander("ğŸ› ë””ë²„ê·¸: ì‚¬ì „ íƒì§€ ì´ìŠˆ"):
    if st.session_state.target_date:
        _, debug_plan = fetch_data(st.session_state.target_date)
        if not debug_plan.empty and 'detected_issues' in debug_plan.columns:
            detected = json.loads(debug_plan['detected_issues'].iloc[0])
            st.json(detected)
    else:
        st.info("ğŸ’¡ ë‚ ì§œê°€ í¬í•¨ëœ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë””ë²„ê·¸ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
