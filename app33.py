
import streamlit as st
import pandas as pd
from supabase import create_client, Client
import requests
import re
from datetime import datetime, timedelta
import json

# 1. Supabase ì„¤ì •
URL = "https://qipphcdzlmqidhrjnjtt.supabase.co"
KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFpcHBoY2R6bG1xaWRocmpuanR0Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjY5NTIwMTIsImV4cCI6MjA4MjUyODAxMn0.AsuvjVGCLUJF_IPvQevYASaM6uRF2C6F-CjwC3eCNVk"

try:
    supabase: Client = create_client(URL, KEY)
except Exception as e:
    st.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
    st.stop()

# 2. ê³ ì • ì„¤ì •
CAPA_INFO = {"ì¡°ë¦½1": 3300, "ì¡°ë¦½2": 3700, "ì¡°ë¦½3": 3600}
CAPA_90_PERCENT = {"ì¡°ë¦½1": 2970, "ì¡°ë¦½2": 3330, "ì¡°ë¦½3": 3240}

WEEKDAY_RULES = {
    "ì¡°ë¦½2": {
        "ì›”ìš”ì¼": ["FAN", "MOTOR"],
        "í™”ìš”ì¼": ["FLANGE", "MOTOR"],
        "ìˆ˜ìš”ì¼": ["FAN", "MOTOR"],
        "ëª©ìš”ì¼": ["FLANGE", "MOTOR"],
        "ê¸ˆìš”ì¼": ["FAN", "MOTOR"],
    }
}

FEW_SHOT_EXAMPLES = """
## ğŸ“š ì°¸ê³ í•  ì„±ê³µ ì‚¬ë¡€

### ì‚¬ë¡€ 1: 2025ë…„ 10ì›” 15ì¼ ì¡°ë¦½2 CAPA ì´ˆê³¼
**í•´ê²°**: ì¡°ë¦½2 â†’ ì¡°ë¦½1ë¡œ 500ê°œ ì´ë™ (PLT 50 ê¸°ì¤€, 10ë°°ìˆ˜), ë‹¬ì„±ë¥  98.5%
**ì¡°ê±´**: ì¡°ë¦½1ì— í•´ë‹¹ í’ˆëª©ì´ ì´ë¯¸ ì¡´ì¬í–ˆìŒ

### ì‚¬ë¡€ 2: 2025ë…„ 11ì›” 8ì¼ ìš”ì¼ê·œì¹™ ìœ„ë°˜
**í•´ê²°**: FAN í’ˆëª©ì„ ëª©ìš”ì¼ â†’ ìˆ˜ìš”ì¼ë¡œ ì´ë™, ë‹¬ì„±ë¥  99.2%
"""

# --- ë°ì´í„° ë¡œë“œ ---
@st.cache_data(ttl=600)
def fetch_data(target_date=None):
    try:
        hist_res = supabase.table("production_issue_analysis_8_11")\
            .select("ìµœì¢…_ì´ìŠˆë¶„ë¥˜, í’ˆëª©ëª…, ë¼ì¸, ë‚ ì§œ, ëˆ„ì ë‹¬ì„±ë¥ ")\
            .execute()
        hist_df = pd.DataFrame(hist_res.data)

        if target_date:
            dt = datetime.strptime(target_date, '%Y-%m-%d')
            start_date = (dt - timedelta(days=5)).strftime('%Y-%m-%d')
            end_date = (dt + timedelta(days=5)).strftime('%Y-%m-%d')
            
            # â­ PLT ì»¬ëŸ¼ í¬í•¨í•´ì„œ ë¡œë“œ
            plan_res = supabase.table("production_plan_2026_01")\
                .select("*")\
                .gte("plan_date", start_date)\
                .lte("plan_date", end_date)\
                .execute()
            plan_df = pd.DataFrame(plan_res.data)
            
            if not plan_df.empty:
                plan_df = analyze_plan_issues(plan_df)
        else:
            plan_df = pd.DataFrame()

        return hist_df, plan_df
    except Exception as e:
        st.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame(), pd.DataFrame()

# --- ì‚¬ì „ ì´ìŠˆ íƒì§€ ---
def analyze_plan_issues(df):
    if df.empty:
        return df
    
    issues = []
    
    for date, group in df.groupby('plan_date'):
        dt = datetime.strptime(date, '%Y-%m-%d')
        weekday = dt.strftime('%A')
        weekday_kr = {'Monday': 'ì›”ìš”ì¼', 'Tuesday': 'í™”ìš”ì¼', 'Wednesday': 'ìˆ˜ìš”ì¼',
                      'Thursday': 'ëª©ìš”ì¼', 'Friday': 'ê¸ˆìš”ì¼', 'Saturday': 'í† ìš”ì¼', 'Sunday': 'ì¼ìš”ì¼'}.get(weekday, weekday)
        
        for line in group['line'].unique():
            line_data = group[group['line'] == line]
            total_qty = line_data['qty_0ì°¨'].sum() if 'qty_0ì°¨' in line_data.columns else 0
            
            if total_qty > CAPA_90_PERCENT.get(line, 9999):
                issues.append({
                    'date': date,
                    'line': line,
                    'issue_type': 'CAPA_ì´ˆê³¼',
                    'current_qty': int(total_qty),
                    'max_qty': CAPA_90_PERCENT[line],
                    'over_qty': int(total_qty - CAPA_90_PERCENT[line])
                })
            
            if line == 'ì¡°ë¦½2' and weekday_kr in WEEKDAY_RULES['ì¡°ë¦½2']:
                allowed_products = WEEKDAY_RULES['ì¡°ë¦½2'][weekday_kr]
                for _, row in line_data.iterrows():
                    product = str(row.get('product_name', ''))
                    is_allowed = any(allowed in product.upper() for allowed in allowed_products)
                    if not is_allowed:
                        issues.append({
                            'date': date,
                            'line': line,
                            'issue_type': 'ìš”ì¼ê·œì¹™_ìœ„ë°˜',
                            'weekday': weekday_kr,
                            'product': product,
                            'allowed': allowed_products,
                            'qty': int(row.get('qty_0ì°¨', 0))
                        })
            
            if line == 'ì¡°ë¦½2' and len(line_data) > 5:
                issues.append({
                    'date': date,
                    'line': line,
                    'issue_type': 'í’ˆëª©ìˆ˜_ì´ˆê³¼',
                    'current_count': len(line_data),
                    'max_count': 5,
                    'products': list(line_data['product_name'].values)
                })
    
    df['detected_issues'] = json.dumps(issues, ensure_ascii=False) if issues else '[]'
    return df

# --- RAG: ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ---
def retrieve_similar_cases(history_df, current_issues):
    if history_df.empty or not current_issues:
        return "ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ"
    
    issue_types = set()
    for issue in current_issues:
        if issue['issue_type'] == 'CAPA_ì´ˆê³¼':
            issue_types.add('CAPA')
        elif issue['issue_type'] == 'ìš”ì¼ê·œì¹™_ìœ„ë°˜':
            issue_types.add('ìš”ì¼')
        elif issue['issue_type'] == 'í’ˆëª©ìˆ˜_ì´ˆê³¼':
            issue_types.add('í’ˆëª©')
    
    similar_cases = []
    for issue_type in issue_types:
        matched = history_df[history_df['ìµœì¢…_ì´ìŠˆë¶„ë¥˜'].str.contains(issue_type, na=False, case=False)]
        if not matched.empty:
            top_cases = matched.nlargest(3, 'ëˆ„ì ë‹¬ì„±ë¥ ') if 'ëˆ„ì ë‹¬ì„±ë¥ ' in matched.columns else matched.head(3)
            similar_cases.append(f"\n### {issue_type} ê´€ë ¨ ê³¼ê±° ì‚¬ë¡€")
            for idx, row in top_cases.iterrows():
                similar_cases.append(f"- ë‚ ì§œ: {row.get('ë‚ ì§œ', 'N/A')}, í’ˆëª©: {row.get('í’ˆëª©ëª…', 'N/A')}, "
                                   f"ë¼ì¸: {row.get('ë¼ì¸', 'N/A')}, ë‹¬ì„±ë¥ : {row.get('ëˆ„ì ë‹¬ì„±ë¥ ', 'N/A')}%")
    
    return "\n".join(similar_cases) if similar_cases else "ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ"

# --- AI ë‹µë³€ ê²€ì¦ ---
def validate_ai_response(response, current_df):
    if current_df.empty:
        return True, [], "âœ… ê²€ì¦í•  ë°ì´í„° ì—†ìŒ"
    
    warnings = []
    details = []
    
    mentioned_dates = set()
    dates_pattern1 = re.findall(r'202[56]-\d{2}-\d{2}', response)
    mentioned_dates.update(dates_pattern1)
    
    dates_pattern2 = re.findall(r'(\d{1,2})/(\d{1,2})', response)
    for m, d in dates_pattern2:
        mentioned_dates.add(f"2026-{int(m):02d}-{int(d):02d}")
    
    dates_pattern3 = re.findall(r'(\d{1,2})ì›”\s*(\d{1,2})ì¼', response)
    for m, d in dates_pattern3:
        mentioned_dates.add(f"2026-{int(m):02d}-{int(d):02d}")
    
    actual_dates = set(current_df['plan_date'].unique())
    invalid_dates = mentioned_dates - actual_dates
    
    if invalid_dates:
        warnings.append({
            'type': 'DATE_MISMATCH',
            'severity': 'MEDIUM',
            'message': f"ë°ì´í„° ë²”ìœ„ ì™¸ ë‚ ì§œ: {', '.join(sorted(invalid_dates))}"
        })
        details.append(f"âš ï¸ **ë‚ ì§œ ì°¸ê³ **: {', '.join(sorted(invalid_dates))} (4ì¼ í›„ ì´ë™ ì œì•ˆ ê°€ëŠ¥)")
    else:
        details.append(f"âœ… **ë‚ ì§œ ê²€ì¦**: í†µê³¼ ({len(mentioned_dates)}ê°œ)")
    
    mentioned_qtys = re.findall(r'\b([1-9]\d{2,})\b', response)
    mentioned_qtys = [int(q) for q in mentioned_qtys]
    
    actual_qtys = set()
    if 'qty_0ì°¨' in current_df.columns:
        actual_qtys = set(current_df['qty_0ì°¨'].dropna().astype(int))
    
    suspicious_qtys = [q for q in mentioned_qtys if q not in actual_qtys and q > 100]
    
    if len(suspicious_qtys) > 3:
        warnings.append({
            'type': 'QUANTITY_SUSPICIOUS',
            'severity': 'MEDIUM',
            'message': f"ì˜ì‹¬ ìˆ˜ëŸ‰ {len(suspicious_qtys)}ê°œ"
        })
        details.append(f"âš ï¸ **ìˆ˜ëŸ‰ ì˜ì‹¬**: ì¼ë¶€ ë¶ˆì¼ì¹˜")
    else:
        details.append(f"âœ… **ìˆ˜ëŸ‰ ê²€ì¦**: í†µê³¼")
    
    after_qtys = re.findall(r'ë³€ê²½\s*í›„\s*ìˆ˜ëŸ‰[:\s]+(\d+)', response)
    after_qtys = [int(q) for q in after_qtys]
    
    capa_violations = []
    for qty in after_qtys:
        for line, max_capa in CAPA_90_PERCENT.items():
            if qty > max_capa:
                capa_violations.append(f"{line} ì´ˆê³¼: {qty} > {max_capa}")
    
    if capa_violations:
        warnings.append({
            'type': 'CAPA_VIOLATION',
            'severity': 'CRITICAL',
            'message': f"CAPA ìœ„ë°˜: {capa_violations[0]}"
        })
        details.append(f"ğŸš¨ **CAPA ìœ„ë°˜**: {capa_violations[0]}")
    else:
        details.append(f"âœ… **CAPA ê²€ì¦**: í†µê³¼")
    
    critical_warnings = [w for w in warnings if w['severity'] == 'CRITICAL']
    high_warnings = [w for w in warnings if w['severity'] == 'HIGH']
    is_valid = len(critical_warnings) == 0 and len(high_warnings) <= 1
    
    validation_report = "\n".join(details)
    if warnings:
        validation_report += "\n\n### âš ï¸ ê²½ê³ \n"
        for w in warnings:
            severity_icon = {'CRITICAL': 'ğŸš¨', 'HIGH': 'âŒ', 'MEDIUM': 'âš ï¸'}.get(w['severity'], 'âš ï¸')
            validation_report += f"{severity_icon} {w['message']}\n"
    
    return is_valid, warnings, validation_report

# --- AI ë¶„ì„ ì—”ì§„ (PLT í¬í•¨) ---
def ask_professional_scheduler(question, current_df, history_df):
    api_url = "https://ai.potens.ai/api/chat"
    api_key = "qD2gfuVAkMJexDAcFb5GnEb1SZksTs7o"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

    if not current_df.empty:
        summary = current_df.groupby(['plan_date', 'line']).agg({
            'qty_0ì°¨': 'sum',
            'product_name': 'count'
        }).reset_index()
        summary.columns = ['plan_date', 'line', 'total_qty', 'product_count']
        
        # â­ PLT ì •ë³´ í¬í•¨
        product_details = current_df.groupby(['plan_date', 'line']).apply(
            lambda x: x[['product_name', 'qty_0ì°¨', 'plt']].to_dict('records')
        ).reset_index()
        product_details.columns = ['plan_date', 'line', 'products']
        
        summary = summary.merge(product_details, on=['plan_date', 'line'])
        
        # â­ í’ˆëª©ë³„ PLT ì •ë³´ ìƒì„±
        product_plt_map = {}
        for _, row in current_df.iterrows():
            product_name = row.get('product_name', '')
            plt = row.get('plt', 1)
            if product_name and plt:
                product_plt_map[product_name] = int(plt)
        
        all_products_by_line = {}
        for line in current_df['line'].unique():
            line_data = current_df[current_df['line'] == line]
            all_products_by_line[line] = sorted(list(line_data['product_name'].unique()))
        
        movement_rules = "\n\n## ğŸšš í’ˆëª© ì´ë™ ê°€ëŠ¥ ì—¬ë¶€ (ì „ì²´ ê¸°ê°„)\n"
        movement_rules += "\n### ğŸ“‹ ë¼ì¸ë³„ ìƒì‚° ê°€ëŠ¥ í’ˆëª© ì „ì²´ ëª©ë¡ (PLT í¬í•¨)\n"
        for line in sorted(all_products_by_line.keys()):
            products = all_products_by_line[line]
            movement_rules += f"\n**{line} ìƒì‚° ê°€ëŠ¥ í’ˆëª© ({len(products)}ê°œ):**\n"
            for prod in products:
                plt_value = product_plt_map.get(prod, '?')
                movement_rules += f"  - {prod} (PLT: {plt_value})\n"
        
        movement_rules += """
âš ï¸ **ì¤‘ìš” ê·œì¹™: í’ˆëª© ë¼ì¸ ì´ë™ ì œì•½**
1. í’ˆëª©ì„ ë‹¤ë¥¸ ë¼ì¸ìœ¼ë¡œ ì´ë™í•˜ë ¤ë©´, **ëª©ì ì§€ ë¼ì¸ì— í•´ë‹¹ í’ˆëª©ì´ ì¡´ì¬**í•´ì•¼ í•©ë‹ˆë‹¤
2. qty_0ì°¨ê°€ 0ì´ì–´ë„ í’ˆëª© í–‰ì´ ì¡´ì¬í•˜ë©´ ì´ë™ ê°€ëŠ¥
3. **ìœ„ ëª©ë¡ì— ì—†ëŠ” í’ˆëª©ìœ¼ë¡œëŠ” ì ˆëŒ€ ì´ë™ ì œì•ˆ ê¸ˆì§€**

ğŸ“¦ **PLT ë°°ìˆ˜ ê·œì¹™ (í•„ìˆ˜!)**
4. **ëª¨ë“  ì´ë™ ìˆ˜ëŸ‰ì€ í•´ë‹¹ í’ˆëª©ì˜ PLT ë°°ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤**
5. ì˜ˆ: PLT 50ì¸ í’ˆëª©ì€ 50, 100, 150, 200... ë‹¨ìœ„ë¡œë§Œ ì´ë™ ê°€ëŠ¥
6. PLT 100ì¸ í’ˆëª©ì€ 100, 200, 300... ë‹¨ìœ„ë¡œë§Œ ì´ë™ ê°€ëŠ¥
7. **PLT ë°°ìˆ˜ê°€ ì•„ë‹Œ ìˆ˜ëŸ‰ ì´ë™ ì ˆëŒ€ ê¸ˆì§€**

ğŸ“… **ë‚ ì§œ ì´ë™ ê·œì¹™**
8. **ë¼ì¸ ê°„ ì´ë™ ì‹œ ë°˜ë“œì‹œ 4ì¼ í›„ë¡œ ì´ë™í•´ì•¼ í•©ë‹ˆë‹¤**
9. ê°™ì€ ë¼ì¸ ë‚´ ë‚ ì§œ ë³€ê²½ì€ ììœ ë¡­ê²Œ ê°€ëŠ¥

âœ… **ì´ë™ ê°€ëŠ¥ ì˜ˆì‹œ:**
- 1/5 ì¡°ë¦½1 "FAN_V710 (PLT:50) 1200ê°œ" â†’ 1/9 ì¡°ë¦½2ë¡œ 500ê°œ ì´ë™ (PLT 50ì˜ 10ë°°ìˆ˜) âœ…
- 1/5 ì¡°ë¦½1 "MOTOR (PLT:100) 800ê°œ" â†’ 1/7 ì¡°ë¦½1ë¡œ 300ê°œ ì´ë™ (PLT 100ì˜ 3ë°°ìˆ˜) âœ…

âŒ **ì´ë™ ë¶ˆê°€ëŠ¥ ì˜ˆì‹œ:**
- 1/5 ì¡°ë¦½1 "FAN (PLT:50)" â†’ 120ê°œ ì´ë™ (PLT ë°°ìˆ˜ ì•„ë‹˜) âŒ
- 1/5 ì¡°ë¦½1 "MOTOR (PLT:100)" â†’ 350ê°œ ì´ë™ (PLT ë°°ìˆ˜ ì•„ë‹˜) âŒ
- 1/5 ì¡°ë¦½1 â†’ 1/6 ì¡°ë¦½2 (4ì¼ í›„ê°€ ì•„ë‹˜) âŒ
"""
        
        data_text = ""
        for _, row in summary.iterrows():
            data_text += f"\n## {row['plan_date']} / {row['line']}\n"
            data_text += f"**ì´ ê³„íš ìˆ˜ëŸ‰: {int(row['total_qty'])}ê°œ** (í’ˆëª© ìˆ˜: {int(row['product_count'])}ê°œ)\n"
            data_text += f"**CAPA 90% ê¸°ì¤€: {CAPA_90_PERCENT.get(row['line'], 'N/A')}ê°œ**\n"
            
            if row['total_qty'] > CAPA_90_PERCENT.get(row['line'], 99999):
                over = int(row['total_qty'] - CAPA_90_PERCENT.get(row['line'], 0))
                data_text += f"âš ï¸ **CAPA ì´ˆê³¼: {over}ê°œ ì´ˆê³¼**\n"
            
            data_text += "\nìƒì„¸ í’ˆëª© (PLT í¬í•¨):\n"
            for prod in row['products']:
                plt_val = prod.get('plt', '?')
                data_text += f"  - {prod['product_name']}: {int(prod['qty_0ì°¨'])}ê°œ (PLT: {plt_val})\n"
        
        detected_issues_str = current_df['detected_issues'].iloc[0] if 'detected_issues' in current_df.columns else '[]'
        detected_issues = json.loads(detected_issues_str)
    else:
        data_text = "ë°ì´í„° ì—†ìŒ"
        movement_rules = ""
        detected_issues = []
    
    similar_cases = retrieve_similar_cases(history_df, detected_issues)
    
    system_rules = f"""
ë‹¹ì‹ ì€ ìë™ì°¨ ë¶€í’ˆ ì¡°ë¦½ë¼ì¸ì˜ 'ìˆ˜ì„ ìƒì‚° ìŠ¤ì¼€ì¤„ëŸ¬'ì…ë‹ˆë‹¤.

## âš ï¸ ì ˆëŒ€ ê·œì¹™
1. **ì•„ë˜ [í˜„ì¬ 1ì›” ê³„íš ë°ì´í„°]ì— ëª…ì‹œëœ "ì´ ê³„íš ìˆ˜ëŸ‰"ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”**
2. ìˆ«ìë¥¼ ì ˆëŒ€ ì„ì˜ë¡œ ê³„ì‚°í•˜ê±°ë‚˜ ì¶”ì •í•˜ì§€ ë§ˆì„¸ìš”
3. ì œê³µëœ ìˆ«ìë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”
4. **í’ˆëª© ì´ë™ ì‹œ ë°˜ë“œì‹œ [í’ˆëª© ì´ë™ ê°€ëŠ¥ ì—¬ë¶€] ì„¹ì…˜ì„ í™•ì¸í•˜ì„¸ìš”**
5. **ëª©ì ì§€ ë¼ì¸ì— ì—†ëŠ” í’ˆëª©ìœ¼ë¡œëŠ” ì ˆëŒ€ ì´ë™ ì œì•ˆ ê¸ˆì§€**
6. **ë¼ì¸ ê°„ ì´ë™ ì‹œ ë°˜ë“œì‹œ 4ì¼ í›„ ë‚ ì§œë¡œ ë°°ì¹˜í•˜ì„¸ìš”**
7. **ëª¨ë“  ì´ë™ ìˆ˜ëŸ‰ì€ í•´ë‹¹ í’ˆëª©ì˜ PLT ë°°ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ (í•„ìˆ˜!)**
8. â­ **ì¡°ë¦½2 ìš”ì¼ ê·œì¹™ì€ ì ˆëŒ€ ìš°ì„ ìˆœìœ„ - ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ ìœ„ë°˜ ê°€ëŠ¥**
   - ëŒ€ì•ˆ 1, 2ì—ì„œëŠ” **ë°˜ë“œì‹œ ìš”ì¼ ê·œì¹™ì„ ì¤€ìˆ˜**í•˜ëŠ” ë°©ë²•ë§Œ ì œì•ˆ
   - ëŒ€ì•ˆ 3(ê¸´ê¸‰ì•ˆ)ì—ì„œë§Œ ì˜ˆì™¸ì ìœ¼ë¡œ ìš”ì¼ ê·œì¹™ ìœ„ë°˜ í—ˆìš©
   - ìš”ì¼ ê·œì¹™ ìœ„ë°˜ ì‹œ ë‹¨ì ì— **"âš ï¸ ì¡°ë¦½2 ìš”ì¼ì œ ìœ„ë°˜ (ìµœí›„ì˜ ìˆ˜ë‹¨)"** ëª…ì‹œ í•„ìˆ˜

## ğŸ“Š í˜„ì¬ 1ì›” ê³„íš ë°ì´í„° (ì •í™•í•œ ì§‘ê³„)
{data_text}

{movement_rules}

## ğŸš¨ ì‚¬ì „ íƒì§€ ì´ìŠˆ
{json.dumps(detected_issues, ensure_ascii=False, indent=2)}

## ğŸ“š ìœ ì‚¬ ê³¼ê±° ì‚¬ë¡€
{similar_cases}

{FEW_SHOT_EXAMPLES}

## ğŸ“ í•„ìˆ˜ ê·œì¹™
1. CAPA 90%: ì¡°ë¦½1={CAPA_90_PERCENT['ì¡°ë¦½1']}, ì¡°ë¦½2={CAPA_90_PERCENT['ì¡°ë¦½2']}, ì¡°ë¦½3={CAPA_90_PERCENT['ì¡°ë¦½3']}
2. ì¡°ë¦½2 ìš”ì¼ì œ: {json.dumps(WEEKDAY_RULES['ì¡°ë¦½2'], ensure_ascii=False)}
3. ì¡°ë¦½2 í’ˆëª©: í•˜ë£¨ ìµœëŒ€ 5í’ˆëª©
4. **í’ˆëª© ì´ë™: ëª©ì ì§€ ë¼ì¸ì— í•´ë‹¹ í’ˆëª©ì´ ì¡´ì¬í•  ë•Œë§Œ ê°€ëŠ¥**
5. **ë¼ì¸ ê°„ ì´ë™ ì‹œ +4ì¼ í›„ ë‚ ì§œë¡œ ë°°ì¹˜ (í•„ìˆ˜)**
6. **ì´ë™ ìˆ˜ëŸ‰ì€ ë°˜ë“œì‹œ PLT ë°°ìˆ˜ (í•„ìˆ˜)**

## ğŸ“ ì¶œë ¥ í˜•ì‹

### ëŒ€ì•ˆ 1: [ì œëª©]

**ğŸ”§ ì¡°ì¹˜ì‚¬í•­**
- ì¶œë°œ: [ë‚ ì§œ] / [ë¼ì¸] / [í’ˆëª©] [ìˆ˜ëŸ‰]ê°œ (PLT: [ê°’])
- ì´ë™ëŸ‰: [PLT ë°°ìˆ˜ ìˆ˜ëŸ‰]ê°œ (PLT [ê°’]ì˜ [N]ë°°)
- ë„ì°©: [ë‚ ì§œ+4ì¼] / [ë„ì°© ë¼ì¸] â† ë¼ì¸ ë³€ê²½ ì‹œ ë°˜ë“œì‹œ +4ì¼
- í’ˆëª© í™•ì¸: âœ… [ë„ì°© ë¼ì¸]ì— [í’ˆëª©ëª…] ì¡´ì¬ í™•ì¸ë¨
- PLT í™•ì¸: âœ… [ì´ë™ëŸ‰]ì€ PLT [ê°’]ì˜ ë°°ìˆ˜ì„

**ğŸ“Š ê·¼ê±°**
- ê·œì¹™: [ë²ˆí˜¸]
- ì´ë™ ê°€ëŠ¥ í™•ì¸: âœ… [ë„ì°© ë¼ì¸]ì— [í’ˆëª©ëª…] ì¡´ì¬í•¨
- PLT ë°°ìˆ˜ í™•ì¸: [ì´ë™ëŸ‰] Ã· [PLT] = [ì •ìˆ˜]
- ë‚ ì§œ ê³„ì‚°: [ì¶œë°œë‚ ì§œ] + 4ì¼ = [ë„ì°©ë‚ ì§œ]
- í˜„ì¬ ìˆ˜ëŸ‰: [ìœ„ ë°ì´í„° ì§ì ‘ ì¸ìš©]

**âœ… ì¥ì  / âš ï¸ ë‹¨ì **

---
(ëŒ€ì•ˆ 2, 3 ë™ì¼)

âš ï¸ ì£¼ì˜: 
1. ìˆ«ìëŠ” ìœ„ [í˜„ì¬ 1ì›” ê³„íš ë°ì´í„°]ì˜ "ì´ ê³„íš ìˆ˜ëŸ‰"ì„ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©
2. í’ˆëª© ì´ë™ì€ [í’ˆëª© ì´ë™ ê°€ëŠ¥ ì—¬ë¶€]ì— ëª…ì‹œëœ í’ˆëª©ë§Œ ê°€ëŠ¥
3. ì´ë™ ì œì•ˆ ì „ ë°˜ë“œì‹œ ëª©ì ì§€ ë¼ì¸ì— í•´ë‹¹ í’ˆëª©ì´ ìˆëŠ”ì§€ í™•ì¸
4. ë¼ì¸ ê°„ ì´ë™ ì‹œ ë°˜ë“œì‹œ +4ì¼ ê³„ì‚°
5. **ì´ë™ ìˆ˜ëŸ‰ì€ ë°˜ë“œì‹œ í•´ë‹¹ í’ˆëª©ì˜ PLT ë°°ìˆ˜ë¡œ ì œì•ˆ**
"""

    payload = {
        "prompt": f"{system_rules}\n\n## ì‚¬ìš©ì ìš”ì²­\n{question}",
        "temperature": 0.1,
        "max_tokens": 3000
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=90)
        response.raise_for_status()
        ai_response = response.json().get('message', 'ì‘ë‹µ ìƒì„± ì˜¤ë¥˜')
        
        is_valid, warnings, validation_report = validate_ai_response(ai_response, current_df)
        
        if not is_valid:
            ai_response += f"\n\n---\n## ğŸ” ê²€ì¦ ê²°ê³¼\n{validation_report}"
        
        return ai_response, is_valid, warnings, validation_report
        
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {str(e)}", False, [], ""

# --- ë‚ ì§œ ì¶”ì¶œ ---
def extract_date(text):
    patterns = [r'(\d{1,2})/(\d{1,2})', r'(\d{1,2})ì›”\s*(\d{1,2})ì¼']
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            m, d = match.groups()
            return f"2026-{int(m):02d}-{int(d):02d}"
    return None

# --- ë©”ì¸ UI ---
st.set_page_config(page_title="AI ìˆ˜ì„ ìŠ¤ì¼€ì¤„ëŸ¬", layout="wide")
st.title("ğŸ‘¨â€âœˆï¸ ìˆ˜ì„ ìŠ¤ì¼€ì¤„ëŸ¬ AI í†µí•© ì „ëµ ê´€ì œ (2026.01)")

with st.sidebar:
    st.header("âš™ï¸ ìƒì‚° ë¼ì¸ CAPA")
    st.json(CAPA_INFO)
    st.subheader("ğŸ“ CAPA 90%")
    st.json(CAPA_90_PERCENT)
    st.subheader("ğŸ“… ì¡°ë¦½2 ìš”ì¼ ê·œì¹™")
    st.json(WEEKDAY_RULES)
    st.info("ğŸ“Œ ë¼ì¸ ê°„ ì´ë™ ì‹œ +4ì¼ í›„ ë°°ì¹˜")
    st.warning("ğŸ“¦ ì´ë™ ìˆ˜ëŸ‰ì€ PLT ë°°ìˆ˜ í•„ìˆ˜")
    if st.button("ğŸ”„ ë°ì´í„° ë™ê¸°í™”"):
        st.cache_data.clear()
        st.rerun()

if "messages" not in st.session_state:
    st.session_state.messages = []

if "target_date" not in st.session_state:
    st.session_state.target_date = None

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]): 
        st.markdown(msg["content"])

if prompt := st.chat_input("ì´ìŠˆë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1/5 ì¡°ë¦½1 CAPA ì´ˆê³¼ í•´ê²°í•´ì¤˜)"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"): 
        st.markdown(prompt)

    target_date = extract_date(prompt)
    st.session_state.target_date = target_date
    
    with st.spinner("ğŸš€ ë¶„ì„ ì¤‘..."):
        history_df, current_plan = fetch_data(target_date)
        answer, is_valid, warnings, validation_report = ask_professional_scheduler(prompt, current_plan, history_df)
        
        st.session_state.messages.append({"role": "assistant", "content": answer})
        
        with st.chat_message("assistant"):
            st.markdown(answer)
            
            if is_valid:
                st.success("âœ… AI ë‹µë³€ ê²€ì¦ í†µê³¼")
            else:
                st.warning("âš ï¸ ì¼ë¶€ ë¶ˆì¼ì¹˜ ë°œê²¬")
            
            with st.expander("ğŸ” ìƒì„¸ ê²€ì¦ ê²°ê³¼"):
                st.markdown(validation_report)
            
            col1, col2 = st.columns(2)
            with col1:
                if not current_plan.empty:
                    with st.expander("ğŸ“ 1ì›” ê³„íš ì›ë³¸"):
                        display_df = current_plan.drop(columns=['detected_issues'], errors='ignore')
                        st.dataframe(display_df)
            
            with col2:
                if not history_df.empty:
                    with st.expander("ğŸ“š ê³¼ê±° ì´ìŠˆ Top 5"):
                        issue_summary = history_df['ìµœì¢…_ì´ìŠˆë¶„ë¥˜'].value_counts().head(5)
                        st.bar_chart(issue_summary)

with st.expander("ğŸ› ë””ë²„ê·¸: ì‚¬ì „ íƒì§€ ì´ìŠˆ ë° í’ˆëª© ì´ë™ ë§¤íŠ¸ë¦­ìŠ¤"):
    if st.session_state.target_date:
        _, debug_plan = fetch_data(st.session_state.target_date)
        if not debug_plan.empty:
            if 'detected_issues' in debug_plan.columns:
                st.subheader("ğŸš¨ íƒì§€ëœ ì´ìŠˆ")
                detected = json.loads(debug_plan['detected_issues'].iloc[0])
                st.json(detected)
            
            st.subheader("ğŸ”„ ë¼ì¸ë³„ í’ˆëª© ëª©ë¡ (PLT í¬í•¨)")
            for line in sorted(debug_plan['line'].unique()):
                line_data = debug_plan[debug_plan['line'] == line]
                products = sorted(line_data['product_name'].unique())
                st.write(f"**{line}** ({len(products)}ê°œ)")
                for prod in products[:10]:
                    plt_val = line_data[line_data['product_name'] == prod]['plt'].iloc[0] if 'plt' in line_data.columns else '?'
                    st.write(f"  - {prod} (PLT: {plt_val})")
    else:
        st.info("ğŸ’¡ ë‚ ì§œê°€ í¬í•¨ëœ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë””ë²„ê·¸ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")


